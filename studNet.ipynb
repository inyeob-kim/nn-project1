{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_train function\n",
      "input size =  8\n",
      "The shape of initial weights\n",
      "[[ 0.17561497 -0.04888381  0.22899248  0.53847237 -0.08278572 -0.08277991\n",
      "   0.55833605  0.27132915]\n",
      " [-0.16598426  0.19182394 -0.1638429  -0.16466033  0.08554658 -0.67644672\n",
      "  -0.60985055 -0.19879866]\n",
      " [-0.35808988  0.11110321 -0.32103499 -0.49932476  0.51818509 -0.07982398\n",
      "   0.02387483 -0.50372455]\n",
      " [-0.19246836  0.03921706 -0.40693768  0.13282931 -0.21235785 -0.10312931\n",
      "  -0.21273541  0.65487923]]\n",
      "[[-0.00674861 -0.52885546  0.41127246 -0.61042182]]\n",
      "The shape of initial biases\n",
      "2\n",
      "feed forward function\n",
      "input = [[0 0]\n",
      " [1 0]\n",
      " [5 1]\n",
      " [5 2]\n",
      " [5 5]\n",
      " [5 6]\n",
      " [0 5]\n",
      " [0 4]]\n",
      "done with forward function\n",
      "[[0.7322921  0.69415384]]\n",
      "RESULT for forward propagation....\n",
      "a\n",
      "[[0.7322921  0.69415384]]\n",
      "pre_activations\n",
      "[array([[ 3.27232333,  4.58403704],\n",
      "       [-3.88512487, -7.44849033],\n",
      "       [-1.2521797 , -0.55651672],\n",
      "       [-2.72390615, -0.08114986]]), array([[1.00628301, 0.81961135]])]\n",
      "activations\n",
      "[array([[0, 0],\n",
      "       [1, 0],\n",
      "       [5, 1],\n",
      "       [5, 2],\n",
      "       [5, 5],\n",
      "       [5, 6],\n",
      "       [0, 5],\n",
      "       [0, 4]]), array([[9.63467037e-01, 9.89889682e-01],\n",
      "       [2.01316531e-02, 5.81981160e-04],\n",
      "       [2.22323051e-01, 3.64353807e-01],\n",
      "       [6.15773600e-02, 4.79723662e-01]]), array([[0.7322921 , 0.69415384]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NeuralNetwork at 0x1fc13ffa208>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    Sigmoid function on a vector this is included for use as your activation function\n",
    "    \n",
    "    Input: a vector of elements to preform sigmoid on\n",
    "    \n",
    "    Output: a vector of elements with sigmoid preformed on them\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoidderiv(z):\n",
    "    '''\n",
    "    The derivative of Sigmoid, you will need this to preform back prop\n",
    "    '''\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "class NeuralNetwork(object):     \n",
    "    '''\n",
    "    This Object outlines a basic neuralnetwork and the methods that it will need to function\n",
    "    \n",
    "    We have included an init method with a size parameter:\n",
    "        Size: A 1D array indicating the node size of each layer\n",
    "            E.G. Size = [2, 4, 1] Will instantiate weights and biases for a network\n",
    "            with 2 input nodes, 1 hidden layer with 4 nodes, and an output layer with 1 node\n",
    "        \n",
    "        test_train defines the sizes of the input and output layers, but the rest is up to your implementation\n",
    "    \n",
    "    In this network for simplicity all nodes in a layer are connected to all nodes in the next layer, and the weights and\n",
    "    biases and intialized as such. E.G. In a [2, 4, 1] network each of the 4 nodes in the inner layer will have 2 weight values\n",
    "    and one biases value.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, size, seed=42):\n",
    "        '''\n",
    "        Here the weights and biases specified above will be instantiated to random values\n",
    "        Your network will change these values to fit a certain dataset by training\n",
    "        '''\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.size = size\n",
    "        self.weights = [np.random.randn(self.size[i], self.size[i-1]) * np.sqrt(1 / self.size[i-1]) for i in range(1, len(self.size))]\n",
    "        self.biases = [np.random.rand(n, 1) for n in self.size[1:]]\n",
    "\n",
    "        print('The shape of initial weights')\n",
    "        print(self.weights[0])\n",
    "        print(self.weights[1])\n",
    "        print('The shape of initial biases')\n",
    "        print(len(self.biases))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Perform a feed forward computation \n",
    "        Parameters:\n",
    "\n",
    "        input: data to be fed to the network with (shape described in spec)\n",
    "\n",
    "        returns:\n",
    "\n",
    "        the output value(s) of each example as ‘a’\n",
    "\n",
    "        The values before activation was applied after the input was weighted as ‘pre_activations’\n",
    "\n",
    "        The values after activation for all layers as ‘activations’\n",
    "\n",
    "        You will need ‘pre_activaitons’ and ‘activations’ for updating the network\n",
    "        '''\n",
    "        print('feed forward function')\n",
    "        print('input =', input)\n",
    "        a = input\n",
    "        pre_activations = []\n",
    "        activations = [a]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = sigmoid(z)\n",
    "            pre_activations.append(z)\n",
    "            activations.append(a)\n",
    "        print('done with forward function')\n",
    "        print(a)\n",
    "        return a, pre_activations, activations\n",
    "\n",
    "    def backpropagate(self, a, pre_activations, activations):\n",
    "        print('backpropagation function')\n",
    "\n",
    "    def train(self, X, y):\n",
    "        a, pre_activations, activations = self.forward(X)\n",
    "        print('RESULT for forward propagation....')\n",
    "        print('a')\n",
    "        print(a)\n",
    "        print('pre_activations')\n",
    "        print(pre_activations)\n",
    "        print('activations')\n",
    "        print(activations)\n",
    "\n",
    "\n",
    "    def predict(self, a):\n",
    "        '''\n",
    "       Input: a: list of list of input vectors to be tested\n",
    "       \n",
    "       This method will test a vector of input parameter vectors of the same form as X in test_train\n",
    "       and return the results (Zero or One) that your trained network came up with for every element.\n",
    "       \n",
    "       This method does this the same way the included forward method moves an input through the network\n",
    "       but without storying the previous values (which forward stores for use with the delta function you must write)\n",
    "        '''\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = sigmoid(z)\n",
    "        predictions = (a > 0.5).astype(int)\n",
    "        return predictions\n",
    "    \n",
    "'''\n",
    "This is the function that we will call to test your network.\n",
    "\n",
    "It must instantiate a network, which we include.\n",
    "\n",
    "It must then train the network given the passed data, where x is the parameters in form:\n",
    "        [[1rst parameter], [2nd parameter], [nth parameter]]\n",
    "    \n",
    "        Where if there are 100 training examples each of the n lists inside the list above will have 100 elements\n",
    "        \n",
    "    Y is the target which is guarenteed to be binary, or in other words true or false:\n",
    "    Y will be of the form: \n",
    "        [[1, 0, 0, ...., 1, 0, 1]]\n",
    "        \n",
    "        (where 1 indicates true and zero indicates false)\n",
    "\n",
    "'''\n",
    "def test_train(X, y):\n",
    "    print('test_train function')\n",
    "    inputSize = np.size(X, 0)\n",
    "    print('input size = ', inputSize)\n",
    "    \n",
    "    #feel free to change the inside (hidden) layers to best suite your implementation\n",
    "    #but the sizes of the input layer and output layer (inputSize and 1) must NOT CHANGE\n",
    "    retNN = NeuralNetwork([inputSize, 4, 1])\n",
    "    #train your network here\n",
    "    retNN.train(X, y)\n",
    "    \n",
    "    #then the function MUST return your TRAINED nueral network\n",
    "    return retNN\n",
    "\n",
    "\n",
    "X = np.array([[0,0],\n",
    "              [1,0],\n",
    "              [5,1],\n",
    "              [5,2],\n",
    "              [5,5],\n",
    "              [5,6],\n",
    "              [0,5],\n",
    "              [0,4]])\n",
    "\n",
    "y = [0,0,1,1,0,0,1,1]\n",
    "\n",
    "test_train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
